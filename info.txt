Extreme gradienbt boosting(XG Boost) is based on the decision tree models.It is very fast and amazinglly accurate, even on default variables.

How t get up in the leader borads?
1. Cross-validation(CV): always sppilt the data into 80:20.
2. Cache your intermdeiate data: Focus on specific step rather than running on the entire data. Almost all python objects can be pickled, but for efficiency always use .save(), and .load() function.
3. Use GridSearchCV: A great module that provide you the set of the variable values.It will try all possible until it finds the optimal set of the values. This is the great automation for optimization. A finely tuned XGBosst can beat the generic neural network in many problems.
4. use the models appropriate to the problems : For eg., for text data use XGBoost or Keras LSTM. FOr Image data, use pre trained keras models(use inception most of the time)
5. Combine models: Try combining various models to gib=ve more accurate information. For e.g inceptonplus the Xception work best for the data(image).
6. Feature extraction : 

Library to use for analysis:

for visusalization , explore seaborn anf matplotlib
for data manipulation, explore Numpy and pandas
for data preprocessing, explore sklearn.preprocesing module

why use XG Boost?

1. Execution Time and Model Performance.
2. It is the ensemble model that create the strong classifer based on the weak classifier.

steps that are important with the datasets

1. Cleaning the data: data scientists sprent 80% of the time in cleanig the data.

> importing the data
> Joining multiple datasets.
> Detecting multiple values.
> Detecting anomalies.
> Imputing for missing values.
> Data quality assurance.

2. Exploratory Data analysis: This is the process of generating question and investigating them with visualization.

An EDA projects should show the follwing skills:
> Ability to formulate relevant questions for investingation
> Identify trends
> Identify covariation between variables
> Communication result effectively using visualization(scatter plot, histogram, box and whisker, etc.)

3. Interacting data visualization

4. Machine Learing : Focus on the projects that have a busisness impact, such as prediction customer churn, fraud detection, or loan deflault.
My ML should convey the follwoing skills:
> Reason why you chose to use specific MAchine learning model.
> Splitting data into training and test set(k-fold cross validation) to avoid overfitting.
> Select the right evaluation metrics(AUC, adj-R^2, confusion matrix, etc)
> Feature engineering and selection
> Hyperparameter tuning.

5. Communication: have the relevant skill to explain the others of what you are doing and why.


## HOW TO IMPROVE THE ACCURACY OF THE TEXT CLASSIFICATION : 
First of all good job done in processing the data and coming up with your base model. I would suggest few things that you can try:

>Improve your model my adding bigrams and tri-grams as features.

>Try doing some topic modelling like latent Dirichlet allocation or Probabilistic latent Semantic Analysis for the corpus using a specified number of topics - say 20. You would get a vector of 20 probabilities corresponding to the 20 topics for each document. You could use that vector as input for your classification or use it as additional features on top of what you already have from your base model enhanced with bigrams and trigrams.

>Another thing I would say is try using a tree based classifier ensemble to capture non-linearity and interactions between the features. Either of random forest or gradient boosting would be fine. In gradient boosting you can use xgboost as its a pretty good package that gives good classification.

>If you are familiar with Deep Learning you can give a try with a Recurrent Neural network architecture(mostly the LSTM versions).

>we can also use of mormalizing the text